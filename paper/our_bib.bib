
@inproceedings{
gansynth,
title={{GANS}ynth: Adversarial Neural Audio Synthesis},
author={Jesse Engel and Kumar Krishna Agrawal and Shuo Chen and Ishaan Gulrajani and Chris Donahue and Adam Roberts},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1xQVn09FX},
}

@misc{wavenet,
      title={WaveNet: A Generative Model for Raw Audio}, 
      author={Aaron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alex Graves and Nal Kalchbrenner and Andrew Senior and Koray Kavukcuoglu},
      year={2016},
      eprint={1609.03499},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@misc{melnet,
      title={MelNet: A Generative Model for Audio in the Frequency Domain}, 
      author={Sean Vasquez and Mike Lewis},
      year={2019},
      eprint={1906.01083},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@ARTICLE{1-nna,
	author = {{Lopez-Paz}, David and {Oquab}, Maxime},
	title = "{Revisiting Classifier Two-Sample Tests}",
	journal = {arXiv e-prints},
	keywords = {Statistics - Machine Learning},
	year = 2016,
	month = oct,
	eid = {arXiv:1610.06545},
	pages = {arXiv:1610.06545},
	archivePrefix = {arXiv},
	eprint = {1610.06545},
	primaryClass = {stat.ML},
	adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161006545L},
	adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{achlioptas,
  title={Learning Representations and Generative Models For 3D Point Clouds},
  author={Achlioptas, Panos and Diamanti, Olga and Mitliagkas, Ioannis and Guibas, Leonidas J},
  journal={arXiv preprint arXiv:1707.02392},
  year={2017}
}

@INPROCEEDINGS{waveglow,  author={R. {Prenger} and R. {Valle} and B. {Catanzaro}},  booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Waveglow: A Flow-based Generative Network for Speech Synthesis},   year={2019},  volume={},  number={},  pages={3617-3621},  doi={10.1109/ICASSP.2019.8683143}}

@InProceedings{nsynth,
  title = 	 {Neural Audio Synthesis of Musical Notes with {W}ave{N}et Autoencoders},
  author =       {Jesse Engel and Cinjon Resnick and Adam Roberts and Sander Dieleman and Mohammad Norouzi and Douglas Eck and Karen Simonyan},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1068--1077},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/engel17a/engel17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/engel17a.html},
  abstract = 	 {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.}
}

@ARTICLE{multi_ch_sourcesep,  author={A. A. {Nugraha} and A. {Liutkus} and E. {Vincent}},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={Multichannel Audio Source Separation With Deep Neural Networks},   year={2016},  volume={24},  number={9},  pages={1652-1664},  doi={10.1109/TASLP.2016.2580946}}

@INPROCEEDINGS{speech_recog,  author={X. {Xiao} and S. {Watanabe} and H. {Erdogan} and L. {Lu} and J. {Hershey} and M. L. {Seltzer} and G. {Chen} and Y. {Zhang} and M. {Mandel} and D. {Yu}},  booktitle={2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Deep beamforming networks for multi-channel speech recognition},   year={2016},  volume={},  number={},  pages={5745-5749},  doi={10.1109/ICASSP.2016.7472778}}

@INPROCEEDINGS{mscoding,  author={J. D. {Johnston} and A. J. {Ferreira}},  booktitle={[Proceedings] ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing},   title={Sum-difference stereo transform coding},   year={1992},  volume={2},  number={},  pages={569-572 vol.2},  doi={10.1109/ICASSP.1992.225993}}

@inproceedings{upmix,
  title={Downmixcompatibe conversion from mono to stereo in time-and frequency-domain},
  author={Fink, Marco and Kraft, Sebastian and Z{\"o}lzer, Udo}, year={2015}, booktitle={in Proc. of the 18th Int. Conference on
Digital Audio Effects} 
}

@INPROCEEDINGS{speech_enh,  author={S. {Araki} and T. {Hayashi} and M. {Delcroix} and M. {Fujimoto} and K. {Takeda} and T. {Nakatani}},  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={Exploring multi-channel features for denoising-autoencoder-based speech enhancement},   year={2015},  volume={},  number={},  pages={116-120},  doi={10.1109/ICASSP.2015.7177943}}

@INPROCEEDINGS{emd,
  author={Y. {Rubner} and C. {Tomasi} and L. J. {Guibas}},
  booktitle={Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)}, 
  title={A metric for distributions with applications to image databases}, 
  year={1998},
  volume={},
  number={},
  pages={59-66},
  doi={10.1109/ICCV.1998.710701}}