\section{Experiments}
\label{sec:experiment}

In this section, we first introduce existing metrics for evaluating sample generation, then propose our new metrics for evaluating stereo image generation by combining existing metrics with \textit{`Side distance' ($D_{side}$) and `Short-time Side Distance (STSD)'}. We then compare the proposed method using M-S channels with previous method using L-R channels.

\subsection{Side distance and Short Time Side Distance}
\label{subsec:s-distance}
\input{assets/figures/stsd}
asdfasdfa ~\ref{fig:stsd}
\subsection{Evaluation metrics}
\label{subsec:metric}
Following prior work, we use earth mover's distance (EMD), proposed by~\cite{emd} to measure the similarity between two distributions. Formally, EMD is defined as follows:
\begin{equation}
    \text{EMD}(s_1, s_2) = \min_{\phi: s_1\to s_2} \sum_{x\in s_1} \|x-\phi(x)\|_2 \nonumber
\end{equation}
where $s_1$ and $s_2$ are two distributions and $\phi$ is a bijection between them. Note that $s_1$ and $s_2$ can be any distribution. One can use STSD of stereo audio as $s_1$ and $s_2$.

Let $S_g$ be the set of generated stereo audios and $S_r$ be the set of reference audios with $|S_g| = |S_r|$. To evaluate generative models, we consider the three metrics, MMD, COV which are introduced by~\cite{achlioptas} and 1-NNA proposed by~\cite{1-nna}.

\begin{itemize}
	\item\textbf{Coverage (COV)} measures the fraction of stereo audios in the reference set that are matched to at least one stereo audio in the generated set. For each stereo audio in the generated set, its nearest neighbor in the reference set is marked as a match:
	\begin{align*}
	\text{COV}(S_g, S_r) = \frac{|\{\arg\min_{Y \in S_r} D_{side}(X,Y) | X \in S_g \}|}{|S_r|},
	\end{align*}
	While coverage is able to detect mode collapse, it does not evaluate the quality of generated stereo audios. 
	\item\textbf{Minimum matching distance (MMD)} is proposed to complement coverage as a metric that measures quality. For each stereo audio in the reference set, the distance to its nearest neighbor in the generated set is computed and averaged:
	\begin{equation}
	\text{MMD}(S_g, S_r) = \frac{1}{|S_r|}\sum_{Y\in S_r} \min_{X\in S_g} D_{side}(X,Y)\,,\nonumber
	\end{equation}
	
	\item \textbf{1-nearest neighbor accuracy (1-NNA)} is proposed by Lopez-Paz and Oquab~\cite{1-nna} for two-sample tests, assessing whether two distributions are identical.
	Let $S_{-X} = S_r \cup S_g - \{X\}$ and $N_X$ be the nearest neighbor of $X$ in $S_{-X}$. $1$-NNA is the leave-one-out accuracy of the 1-NN classifier:
	\begin{align*}
	\text{1-NNA}(&S_g, S_r) \\
	=&\frac{\sum_{X\in S_g} \mathbbm{1}[N_X \in S_g] +  \sum_{Y\in S_r} \mathbbm{1}[N_Y \in S_r]}{|S_g|+|S_r|} ,
	\end{align*}
	where $\mathbbm{1}[\cdot]$ is the indicator function.
	For each sample, the 1-NN classifier classifies it as coming from $S_r$ or $S_g$ according to the label of its nearest sample.
	If $S_g$ and $S_r$ are sampled from the same distribution, the accuracy of such a classifier should converge to $50\%$ given a sufficient number of samples. The closer the accuracy is to $50\%$, the more similar $S_g$ and $S_r$ are, and therefore the better the model is at learning the target distribution.
\end{itemize}

\subsection{Experimental results}
\label{subsec:result}
\subsubsection{Dataset and implementation details}
\label{subsubsec:dataset}

\subsubsection{Results}
\label{subsubsec:result}
\input{assets/tables/result}