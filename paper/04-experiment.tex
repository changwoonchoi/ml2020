\section{Experiments}
\label{sec:experiment}

In this section, we first propose our novel representation of stereo audio, \textit{`Side distance' ($D_{side}$) and `Short-time Side Distance (STSD)'}. Next, we introduce existing metrics for evaluating sample generation, then propose our new metrics for evaluating stereo image generation by combining existing metrics with side distance and STSD. We then compare the proposed method using M-S channels with the previous method(GANSynth~\cite{gansynth}) which consumes L-R channels.

\subsection{Representation for stereo audio}
\label{subsec:representation}
The goal of the generative model is to learn to produce samples that look similar to the ones on which it has been trained. Therefore, in order to evaluate the generative model, a proper distance metric between the generated samples and the samples from the dataset is essential. Until now, however, a metric for measuring the similarity between stereo images has never been suggested. In this section, we propose a novel representation of stereo audio that allows us to define the distance metric between them.

\subsubsection{Side distance}
\label{subsubsec:side_distance}
\input{assets/figures/stsd}
When $y$ is a stereo audio of length $T$, we define \textit{`side distance'} $D_{side}$ as following:
\begin{equation}
    D_{side}(y) = {\frac{\sqrt{2}}{2}}(\max_{t \in [0, T)} [y_L(t) - y_R(t)] - \min_{t \in [0, T)} [y_L(t) - y_R(t)])
\end{equation}
where $y_L$ and $y_R$ are left and right channel of stereo audio $y$. For robustness, we use the 0.95, 0.05 quantiles of $y_L(t) - y_R(t)$ instead of maximum and minimum. The side distance can be viewed as an index indicating how far the given stereo audio is spread as visualized in Fig.~\ref{subfig:SD}

\subsubsection{Short-time Side Distance (STSD)}
\label{subsubsec:stsd}
Although side distance is an indicator of how wide the given stereo audio is spread in the auditory space, it cannot express the change of the stereo image over time. Therefore, we introduce the concept of \textit{`Short-time Side Distance (STSD)'} to capture the characteristics of the stereo image over time. The STSD is a sequence of side distance of a windowed signal. The procedure to obtain STSD is to divide a longer time signal into shorter segments of equal length and then compute side distance on each shorter segment. Formally,
\begin{equation}
    STFT(y(t)) = D_{side}(y(\tau)w(t))
\end{equation}
where $w(t)$ is a window function which is nonzero for short period of time. As depicted in Fig.\ref{subfig:STSD}, STSD contains the change in the side distance of the windowed short fragment of signal over time.
\subsection{Evaluation metrics}
\label{subsec:metric}
Following prior work, we use earth mover's distance (EMD), proposed by~\cite{emd} to measure the similarity between two stereo audios' STSDs. Formally, EMD is defined as follows:
\begin{equation}
    \text{EMD}(s_1, s_2) = \min_{\phi: s_1\to s_2} \sum_{x\in s_1} \|x-\phi(x)\|_2 \nonumber
\end{equation}
where $s_1$ and $s_2$ are two distributions and $\phi$ is a bijection between them. Note that $s_1$ and $s_2$ can be any distribution. One can use STSD of stereo audio as $s_1$ and $s_2$.

Let $S_g$ be the set of generated stereo audios and $S_r$ be the set of reference audios with $|S_g| = |S_r|$. To evaluate generative models, we consider the three metrics, MMD, COV which are introduced by~\cite{achlioptas} and 1-NNA proposed by~\cite{1-nna}.

\begin{itemize}
	\item\textbf{Coverage (COV)} measures the fraction of stereo audios in the reference set that are matched to at least one stereo audio in the generated set. For each stereo audio in the generated set, its nearest neighbor in the reference set is marked as a match:
	\begin{align*}
	\text{COV}(S_g, S_r) = \frac{|\{\arg\min_{Y \in S_r} D(X,Y) | X \in S_g \}|}{|S_r|},
	\end{align*}
	where $D(X,Y)$ is a distance metric between two stereo audios.
	While coverage can detect mode collapse, it does not evaluate the quality of generated stereo audios. 
	\item\textbf{Minimum matching distance (MMD)} is proposed to complement coverage as a metric that measures quality. For each stereo audio in the reference set, the distance to its nearest neighbor in the generated set is computed and averaged:
	\begin{equation}
	\text{MMD}(S_g, S_r) = \frac{1}{|S_r|}\sum_{Y\in S_r} \min_{X\in S_g} D(X,Y),\nonumber
	\end{equation}
	where $D(X,Y)$ is a distance metric between two stereo audios.
	\item \textbf{1-nearest neighbor accuracy (1-NNA)} is proposed by Lopez-Paz and Oquab~\cite{1-nna} for two-sample tests, assessing whether two distributions are identical.
	Let $S_{-X} = S_r \cup S_g - \{X\}$ and $N_X$ be the nearest neighbor of $X$ in $S_{-X}$. $1$-NNA is the leave-one-out accuracy of the 1-NN classifier with given distance metric:
	\begin{align*}
	\text{1-NNA}(&S_g, S_r) \\
	=&\frac{\sum_{X\in S_g} \mathbbm{1}[N_X \in S_g] +  \sum_{Y\in S_r} \mathbbm{1}[N_Y \in S_r]}{|S_g|+|S_r|} ,
	\end{align*}
	where $\mathbbm{1}[\cdot]$ is the indicator function.
	For each sample, the 1-NN classifier classifies it as coming from $S_r$ or $S_g$ according to the label of its nearest sample.
	If $S_g$ and $S_r$ are sampled from the same distribution, the accuracy of such a classifier should converge to $50\%$ given a sufficient number of samples. The closer the accuracy is to $50\%$, the more similar $S_g$ and $S_r$ are, and therefore the better the model is at learning the target distribution.
\end{itemize}
As a definition of COV, MMD, 1-NNA, and $D(X,Y)$ can be any distance metric between two audio samples. We use EMD(STSD($X$), STSD($Y$)) as a distance metric $D(X,Y)$.

\subsection{Experimental results}
\label{subsec:result}
\if 0
\subsubsection{Dataset}
\label{subsubsec:dataset}
In this work, we propose a new dataset contains stereo audios with rich stereo images. The dataset consists of EDM stabs with 400ms time duration. Further details can be found in Appendix~\ref{sec:dataset_detail}. 
\fi

\subsubsection{Quantitative results}
\label{subsubsec:result}
\input{assets/tables/result}
For a fair comparison, we trained both our model and GANSynth~\cite{gansynth} with exactly same the hyperparameters (including network architectures, learning rate, epochs) in our training set. The only difference during the training scheme between GANSynth~\cite{gansynth} and the proposed method is the channel encoding. Our proposed method consumes two-channel audio that is reparameterized from L-R representation to M-S representation while GANSynth~\cite{gansynth} takes input as raw L-R represented stereo audios. The quantitative results are reported in Table~\ref{table:result}. We observe that it is improved significantly by simply reparameterize the stereo audio in terms of every evaluation metrics (COV, MMD, 1-NNA).

\subsubsection{Comparing audio quality to concurrent work}
\label{subsubsec:audio_quality}
\input{assets/figures/quality_result}
To better compare our method with concurrent work, we perform a subjective analysis over the stereo audios generated by both methods. In Fig.~\ref{fig:quality_result}, we show the percentages of participants based on how they voted for the plausibility comparisons between ours and GANSynth~\cite{gansynth}. The study employed 15 participants with different backgrounds - 5 audio researchers, 10 non-technical subjects. The participants are asked to choose the better audio between ours and other work. We can observe that our method received better results on both the audio researcher group and the non-technical subject group.