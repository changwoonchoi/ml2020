\section{Related Work}
\label{sec:related_work}
\subsection{Audio generative model}
The earliest generated models for audio tend to focus on speech synthesis. In this case, models require handling variable length, and WaveNet~\cite{wavenet} used the autoregressive method for variable-length inputs and outputs. A flow-based WaveGlow~\cite{waveglow} has emerged that compensates for slow speed of autoregressive models. In comparison to speech, audio generation for music is relatively in the development stage. ~\cite{nsynth} proposed generation of a single musical note to use WaveNet, but it is still slow, and global latent conditioning was impossible. GANSynth~\cite{gansynth} with Generative Adversarial Network solves these shortcomings and provides high-fidelity and locally-coherent audio represented with log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. As a result of the research to date, it is possible to learn a single note of a variety of instruments to convincingly describe the sound of real instruments and to create a variety of synthesizers by interpolation between two instruments. Despite such breakthrough development, the reason that the generated musical notes cannot be used for commercial music is that the generated result is a single channel. Despite this breakthrough, the generated musical notes cannot replace commonly used virtual instruments due to the limitation of a single channel.

\subsection{Multi-channel audio}

In the machine listening community, multi-channel audios are mainly used for speech separation~\cite{multi_ch_sourcesep}, speech enhancement~\cite{speech_enh}, and speech recognition~\cite{speech_recog} in the light of their ability to utilize information about speech source location. On the other hand,~\cite{upmix} proposed an upmixing conversion of the mono signal to pseudo-stereo in order to enhance the audio effect. However, to the best of our knowledge, there are still no attempts to generate high-fidelity multi-channel audio with a neural network. It is necessary to learn the difference while maintaining the coherency of both channels to form a spatiality of sound. A technique referred to as “mid-side coding” exploits the common part of a stereophonic input signal by encoding the sum and difference signals of the two input signals rather than the input signals themselves. (~\cite{mscoding} described the concept of mid-side coding of using the interchannel redundancies.) Therefore, for a high-performance multi-channel audio generative model, we would like to train the state-of-the-art audio generative model(~\cite{gansynth}, so-called GANSynth) through the mid-side coding to verify whether this attempt can effectively learn stereo image.