\section{Reference}

\begin{frame}{Reference}
    \begin{block}{}
    \footnotesize
    {\fontsize{8}{10}{
        $[1]$ Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas Eck, Karen Simonyan, and Mohammad Norouzi. Neural audio synthesis of musical notes with WaveNet autoencoders. In ICML, 2017.\\
        %\midskip
        $[2]$ Chris Donahue, Julian McAuley, and Miller Puckette. Adversarial audio synthesis. In ICLR, 2019.\\
        %\midskip
        $[3]$ Jesse  Engel,   Kumar  Krishna  Agrawal,   Shuo  Chen,Ishaan  Gulrajani,  Chris  Donahue,  and  Adam  Roberts,“GANSynth:  Adversarial  neural  audio  synthesis,”   inInternational Conference on Learning Representations,2019.\\
        %\midskip
        $[4]$ A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichan-nel audio source separation with deep neural networks,”IEEE/ACM Transactions on Audio, Speech, and Lan-guage Processing, vol. 24, no. 9, pp. 1652–1664, 2016.\\
        %\midskip
        $[5]$ S.   Araki,   T.   Hayashi,   M.   Delcroix,   M.   Fujimoto,K. Takeda, and T. Nakatani,  “Exploring multi-channelfeatures  for  denoising-autoencoder-based  speech  en-hancement,”    in2015 IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP),2015, pp. 116–120.\\
        %\midskip
        $[6]$ X. Xiao, S. Watanabe, H. Erdogan, L. Lu, J. Hershey,M.  L.  Seltzer,  G.  Chen,  Y.  Zhang,  M.  Mandel,  andD. Yu,  “Deep beamforming networks for multi-channelspeech recognition,”  in2016 IEEE International Con-ference on Acoustics, Speech and Signal Processing(ICASSP), 2016, pp. 5745–5749.
        }}
    \end{block}
\end{frame}